# models/storyboard_unet.py
"""
Storyboard UNet with Consistent Self-Attention
ë°°ì¹˜ ë‚´ ìƒí˜¸ ì°¸ì¡° ë°©ì‹ìœ¼ë¡œ ìºë¦­í„° ì¼ê´€ì„± ìœ ì§€
"""

import torch
import torch.nn as nn
from diffusers import UNet2DConditionModel
from .consistent_attention import (
    set_consistent_attention,
    remove_consistent_attention,
)


class StoryboardUNet(nn.Module):
    """
    Audio/Text conditioningì„ ë°›ëŠ” UNet
    + Consistent Self-Attention for character consistency across frames
    
    ì…ë ¥: [B * num_frames, C, H, W] í˜•íƒœì˜ Flat Batch
    ë‚´ë¶€ì—ì„œ Consistent Self-Attentionì´ ìë™ìœ¼ë¡œ í”„ë ˆì„ ê°„ ì°¸ì¡° ìˆ˜í–‰
    
    Conditioning modes:
    - "audio": Audio embeddingë§Œ ì‚¬ìš©
    - "text": Text embeddingë§Œ ì‚¬ìš© (ablation)
    - "both": Audio + Text fusion
    """
    
    def __init__(
        self,
        pretrained_model: str = "runwayml/stable-diffusion-v1-5",
        freeze_unet: bool = True,
        use_consistent_attention: bool = True,
        num_frames: int = 4,
        attention_mode: str = "first",  # "first" or "mutual"
    ):
        super().__init__()
        
        self.num_frames = num_frames
        self.use_consistent_attention = use_consistent_attention
        
        # 1. Load UNet
        self.unet = UNet2DConditionModel.from_pretrained(
            pretrained_model,
            subfolder="unet"
        )
        
        self.cross_attention_dim = self.unet.config.cross_attention_dim
        
        # 2. Freeze UNet
        if freeze_unet:
            self.unet.requires_grad_(False)
            print("ğŸ”’ U-Net frozen")
        else:
            print("ğŸ”“ U-Net unfrozen")
        
        # 3. Inject Consistent Self-Attention (StoryDiffusion Core)
        if use_consistent_attention:
            set_consistent_attention(
                self.unet,
                num_frames=num_frames,
                attention_mode=attention_mode,
            )
    
    def enable_consistent_attention(self, attention_mode: str = "first"):
        """Consistent Attention í™œì„±í™”"""
        if not self.use_consistent_attention:
            set_consistent_attention(
                self.unet,
                num_frames=self.num_frames,
                attention_mode=attention_mode,
            )
            self.use_consistent_attention = True
    
    def disable_consistent_attention(self):
        """Consistent Attention ë¹„í™œì„±í™” (Ablationìš©)"""
        if self.use_consistent_attention:
            remove_consistent_attention(self.unet)
            self.use_consistent_attention = False
    
    def forward(
        self,
        sample: torch.Tensor,        # [B * num_frames, 4, H, W] - Flat Batch
        timestep: torch.Tensor,      # [B * num_frames]
        audio_embeds: torch.Tensor = None,  # [B * num_frames, 77, 768]
        text_embeds: torch.Tensor = None,   # [B * num_frames, 77, 768]
        conditioning_mode: str = "audio"    # "audio", "text", "both"
    ) -> torch.Tensor:
        """
        Args:
            sample: Noisy latent [B * num_frames, 4, H, W]
            timestep: Diffusion timestep [B * num_frames]
            audio_embeds: Audio encoder output [B * num_frames, 77, 768]
            text_embeds: CLIP text embedding [B * num_frames, 77, 768]
            conditioning_mode: "audio", "text", or "both"
        
        Returns:
            noise_pred: [B * num_frames, 4, H, W]
        
        Note:
            Consistent Self-Attentionì´ ë‚´ë¶€ì ìœ¼ë¡œ ì‘ë™í•˜ì—¬
            ë°°ì¹˜ ì•ˆì—ì„œ ì²« ë²ˆì§¸ í”„ë ˆì„(index 0, num_frames, 2*num_frames, ...)ì˜
            ì •ë³´ë¥¼ ë‚˜ë¨¸ì§€ í”„ë ˆì„ë“¤ì´ ì°¸ì¡°í•˜ê²Œ ë¨
        """
        
        # Conditioning Fusion
        if conditioning_mode == "audio":
            if audio_embeds is None:
                raise ValueError("audio_embeds required for 'audio' mode")
            encoder_hidden_states = audio_embeds
            
        elif conditioning_mode == "text":
            if text_embeds is None:
                raise ValueError("text_embeds required for 'text' mode")
            encoder_hidden_states = text_embeds
            
        elif conditioning_mode == "both":
            if audio_embeds is None or text_embeds is None:
                raise ValueError("Both audio_embeds and text_embeds required for 'both' mode")
            encoder_hidden_states = (audio_embeds + text_embeds) / 2
            
        else:
            raise ValueError(f"Unknown conditioning_mode: {conditioning_mode}")
        
        # UNet Forward
        # ë‚´ë¶€ì ìœ¼ë¡œ ConsistentSelfAttentionProcessorê°€ ì‘ë™í•˜ì—¬
        # ë°°ì¹˜ ì•ˆì—ì„œ í”„ë ˆì„ ê°„ ì •ë³´ ê³µìœ 
        noise_pred = self.unet(
            sample,
            timestep,
            encoder_hidden_states=encoder_hidden_states
        ).sample
        
        return noise_pred
