# models/storyboard_unet.py
import torch
import torch.nn as nn
from diffusers import UNet2DConditionModel

class StoryboardUNet(nn.Module):
    """
    Audio embeddingì´ Text embeddingì„ ëŒ€ì²´í•˜ëŠ” êµ¬ì¡°
    
    í•µì‹¬: Audio Encoderê°€ [B, 77, 768] ì¶œë ¥ â†’ ê¸°ì¡´ textì™€ ë™ì¼í•œ í˜•íƒœ
    â†’ Frozen UNetì´ í•´ì„ ê°€ëŠ¥
    """
    
    def __init__(
        self,
        pretrained_model: str = "runwayml/stable-diffusion-v1-5",
        freeze_unet: bool = True
    ):
        super().__init__()
        
        self.unet = UNet2DConditionModel.from_pretrained(
            pretrained_model,
            subfolder="unet"
        )
        
        self.cross_attention_dim = self.unet.config.cross_attention_dim
        
        if freeze_unet:
            self.unet.requires_grad_(False)
            print("ğŸ”’ U-Net frozen")
        else:
            print("ğŸ”“ U-Net unfrozen")
    
    def forward(
        self,
        sample: torch.Tensor,
        timestep: torch.Tensor,
        audio_embeds: torch.Tensor,      # [B, 77, 768] - textì™€ ê°™ì€ shape
        text_embeds: torch.Tensor = None  # [B, 77, 768] - optional (fusionìš©)
    ) -> torch.Tensor:
        """
        Args:
            sample: Noisy latent [B, 4, H, W]
            timestep: Diffusion timestep [B]
            audio_embeds: Audio encoder output [B, 77, 768]
            text_embeds: CLIP text embedding [B, 77, 768] (optional)
        """
        
        if text_embeds is not None:
            # Audio + Text fusion (element-wise addition or learned gate)
            # ë‹¨ìˆœ í‰ê·  ë˜ëŠ” ê°€ì¤‘í•©
            encoder_hidden_states = (audio_embeds + text_embeds) / 2
        else:
            # Audio only
            encoder_hidden_states = audio_embeds
        
        noise_pred = self.unet(
            sample,
            timestep,
            encoder_hidden_states=encoder_hidden_states
        ).sample
        
        return noise_pred