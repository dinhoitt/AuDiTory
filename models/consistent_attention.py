# models/consistent_attention.py
"""
Consistent Self-Attention for Storyboard Generation
Based on StoryDiffusion: https://github.com/HVision-NKU/StoryDiffusion

ë°°ì¹˜ ë‚´ ìƒí˜¸ ì°¸ì¡°(Batch Mutual Attention) ë°©ì‹:
- 4ì¥ì˜ í”„ë ˆì„ì´ ë™ì‹œì— ìƒì„±ë  ë•Œ, ì²« ë²ˆì§¸ í”„ë ˆì„ì„ ì°¸ì¡°í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
- Write/Read ëª¨ë“œ ì—†ì´ ìë™ìœ¼ë¡œ ì‘ë™
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional


class ConsistentSelfAttentionProcessor(nn.Module):
    """
    StoryDiffusion Style: Batch-wise Consistent Self-Attention
    
    ë°°ì¹˜ ë‚´ì˜ ë‹¤ë¥¸ í”„ë ˆì„(ì´ë¯¸ì§€)ë“¤ì„ ì„œë¡œ ì°¸ì¡°í•˜ê²Œ í•¨.
    ëª¨ë“  í”„ë ˆì„ì€ ìê¸° ìì‹  + ì²« ë²ˆì§¸ í”„ë ˆì„(Reference)ì„ ì°¸ì¡°.
    
    Input shape: [B * num_frames, SeqLen, Dim]
    """
    
    def __init__(
        self,
        num_frames: int = 4,
        attention_mode: str = "first",  # "first" (ì²« ì¥ ì°¸ì¡°) or "mutual" (ì„œë¡œ ì°¸ì¡°)
        device: str = "cuda",
        dtype: torch.dtype = torch.float16,
    ):
        super().__init__()
        self.num_frames = num_frames
        self.attention_mode = attention_mode
        self.device = device
        self.dtype = dtype

    def __call__(
        self,
        attn,
        hidden_states: torch.Tensor,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        temb: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> torch.Tensor:
        """
        Args:
            attn: Attention module from diffusers
            hidden_states: [B * num_frames, SeqLen, Dim]
            encoder_hidden_states: Not used for self-attention
            attention_mask: Optional attention mask
            temb: Optional timestep embedding
        """
        residual = hidden_states
        
        # Spatial norm if exists
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        
        input_ndim = hidden_states.ndim
        
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(
                batch_size, channel, height * width
            ).transpose(1, 2)
        
        # hidden_states: [B * num_frames, SeqLen, Dim]
        batch_size_total, sequence_length, dim = hidden_states.shape
        batch_size = batch_size_total // self.num_frames
        
        # Group norm if exists
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        
        # 1. Q, K, V Projection
        query = attn.to_q(hidden_states)
        key = attn.to_k(hidden_states)
        value = attn.to_v(hidden_states)
        
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        
        # Reshape for multi-head attention
        # [B*N, Seq, Dim] -> [B*N, Heads, Seq, HeadDim]
        query = query.view(batch_size_total, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size_total, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size_total, -1, attn.heads, head_dim).transpose(1, 2)
        
        # =========================================================================
        # ğŸ”¥ í•µì‹¬: StoryDiffusion Logic (Consistent Self-Attention)
        # =========================================================================
        # ì „ëµ: "ëª¨ë“  í”„ë ˆì„(Nê°œ)ì€ ìê¸° ìì‹  + ì²« ë²ˆì§¸ í”„ë ˆì„(Ref)ì„ ë³¸ë‹¤"
        # Key/Valueë¥¼ [B, N, Heads, Seq, Dim] í˜•íƒœë¡œ ë¶„ë¦¬í•˜ì—¬ ì¡°ì‘
        
        # [B*N, Heads, Seq, Dim] -> [B, N, Heads, Seq, Dim]
        key_reshaped = key.view(batch_size, self.num_frames, attn.heads, -1, head_dim)
        value_reshaped = value.view(batch_size, self.num_frames, attn.heads, -1, head_dim)
        
        if self.attention_mode == "first":
            # Reference Key/Value (ì²« ë²ˆì§¸ í”„ë ˆì„) ì¶”ì¶œ: [B, 1, Heads, Seq, Dim]
            key_ref = key_reshaped[:, 0:1]
            value_ref = value_reshaped[:, 0:1]
            
            # ëª¨ë“  í”„ë ˆì„ì— ëŒ€í•´ ë¸Œë¡œë“œìºìŠ¤íŠ¸: [B, N, Heads, Seq, Dim]
            key_ref = key_ref.expand(-1, self.num_frames, -1, -1, -1)
            value_ref = value_ref.expand(-1, self.num_frames, -1, -1, -1)
            
        elif self.attention_mode == "mutual":
            # ëª¨ë“  í”„ë ˆì„ì˜ í‰ê· ì„ ì°¸ì¡°
            key_ref = key_reshaped.mean(dim=1, keepdim=True).expand(-1, self.num_frames, -1, -1, -1)
            value_ref = value_reshaped.mean(dim=1, keepdim=True).expand(-1, self.num_frames, -1, -1, -1)
        
        # ë‹¤ì‹œ ë³‘í•©: [B, N, Heads, Seq, Dim] -> [B*N, Heads, Seq, Dim]
        key_ref = key_ref.reshape(batch_size_total, attn.heads, -1, head_dim)
        value_ref = value_ref.reshape(batch_size_total, attn.heads, -1, head_dim)
        
        # Original Key/Valueì™€ Reference Key/Value ê²°í•© (Concatenate)
        # ê²°ê³¼: ê° í† í°ì€ "ìê¸° ìì‹ ì˜ ì •ë³´" + "ì²« ì»·ì˜ ì •ë³´"ë¥¼ ë™ì‹œì— ë´„
        # Key shape: [B*N, Heads, Seq + Seq, Dim]
        key_combined = torch.cat([key_ref, key], dim=2)
        value_combined = torch.cat([value_ref, value], dim=2)
        
        # =========================================================================
        
        # 3. Scaled Dot-Product Attention
        hidden_states = F.scaled_dot_product_attention(
            query, key_combined, value_combined,
            attn_mask=None,
            dropout_p=0.0,
            is_causal=False
        )
        
        # Reshape back
        hidden_states = hidden_states.transpose(1, 2).reshape(
            batch_size_total, -1, inner_dim
        )
        hidden_states = hidden_states.to(query.dtype)
        
        # Linear projection
        hidden_states = attn.to_out[0](hidden_states)
        # Dropout
        hidden_states = attn.to_out[1](hidden_states)
        
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(
                batch_size, channel, height, width
            )
        
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        
        hidden_states = hidden_states / attn.rescale_output_factor
        
        return hidden_states


class StandardAttentionProcessor(nn.Module):
    """Standard attention processor (no consistent attention)"""
    
    def __init__(self):
        super().__init__()
    
    def __call__(
        self,
        attn,
        hidden_states: torch.Tensor,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        temb: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> torch.Tensor:
        
        residual = hidden_states
        
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        
        input_ndim = hidden_states.ndim
        
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(
                batch_size, channel, height * width
            ).transpose(1, 2)
        
        batch_size, sequence_length, _ = hidden_states.shape
        
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(
                attention_mask, sequence_length, batch_size
            )
            attention_mask = attention_mask.view(
                batch_size, attn.heads, -1, attention_mask.shape[-1]
            )
        
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        
        query = attn.to_q(hidden_states)
        
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        
        hidden_states = F.scaled_dot_product_attention(
            query, key, value,
            attn_mask=attention_mask,
            dropout_p=0.0,
            is_causal=False
        )
        
        hidden_states = hidden_states.transpose(1, 2).reshape(
            batch_size, -1, attn.heads * head_dim
        )
        hidden_states = hidden_states.to(query.dtype)
        
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(
                batch_size, channel, height, width
            )
        
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        
        hidden_states = hidden_states / attn.rescale_output_factor
        
        return hidden_states


def set_consistent_attention(
    unet,
    num_frames: int = 4,
    attention_mode: str = "first",
    device: str = "cuda",
    dtype: torch.dtype = torch.float16,
) -> int:
    """
    UNetì˜ Self-Attention ë ˆì´ì–´ì— Consistent Self-Attention ì ìš©
    
    Args:
        unet: UNet2DConditionModel
        num_frames: ìŠ¤í† ë¦¬ë³´ë“œ í”„ë ˆì„ ìˆ˜
        attention_mode: "first" (ì²« ì¥ ì°¸ì¡°) or "mutual" (ìƒí˜¸ ì°¸ì¡°)
        device: Device
        dtype: Data type
    
    Returns:
        ì ìš©ëœ í”„ë¡œì„¸ì„œ ìˆ˜
    """
    attn_procs = {}
    consistent_count = 0
    
    for name in unet.attn_processors.keys():
        # Self-attention (attn1) vs Cross-attention (attn2) êµ¬ë¶„
        cross_attention_dim = (
            None if name.endswith("attn1.processor")
            else unet.config.cross_attention_dim
        )
        
        if cross_attention_dim is None:
            # Self-attention ë ˆì´ì–´
            # Up-blocksì—ë§Œ ì ìš© (ë…¼ë¬¸ì—ì„œ ê¶Œì¥í•˜ëŠ” ë°©ì‹)
            if "up_blocks" in name:
                attn_procs[name] = ConsistentSelfAttentionProcessor(
                    num_frames=num_frames,
                    attention_mode=attention_mode,
                    device=device,
                    dtype=dtype,
                )
                consistent_count += 1
            else:
                attn_procs[name] = StandardAttentionProcessor()
        else:
            # Cross-attention ë ˆì´ì–´ - í‘œì¤€ í”„ë¡œì„¸ì„œ ì‚¬ìš©
            attn_procs[name] = StandardAttentionProcessor()
    
    unet.set_attn_processor(attn_procs)
    print(f"âœ… Consistent Self-Attention ì ìš© ì™„ë£Œ (up_blocks, {consistent_count}ê°œ)")
    
    return consistent_count


def remove_consistent_attention(unet):
    """Consistent Attentionì„ ì œê±°í•˜ê³  í‘œì¤€ Attentionìœ¼ë¡œ ë³µì›"""
    attn_procs = {}
    
    for name in unet.attn_processors.keys():
        attn_procs[name] = StandardAttentionProcessor()
    
    unet.set_attn_processor(attn_procs)
    print("âœ… Standard Attentionìœ¼ë¡œ ë³µì› ì™„ë£Œ")
