# train.py
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import LambdaLR
import yaml
import os
import glob
import re
import argparse
from tqdm import tqdm
import wandb
import math

from data.dataset import PreprocessedStoryboardDataset, collate_fn
from models.pipeline import AudioToStoryboardPipeline


def scan_checkpoint(checkpoint_dir, prefix):
    """ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°"""
    pattern = os.path.join(checkpoint_dir, prefix + '*')
    checkpoints = glob.glob(pattern)
    if len(checkpoints) == 0:
        return None
    def extract_number(path):
        numbers = re.findall(r'\d+', os.path.basename(path))
        return int(numbers[-1]) if numbers else 0
    checkpoints.sort(key=extract_number)
    return checkpoints[-1]


def load_checkpoint(checkpoint_path, model, optimizer=None, device='cuda'):
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë° ìƒíƒœ ë³µì›"""
    print(f"ğŸ“‚ Loading checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    model.audio_encoder.load_state_dict(checkpoint['audio_encoder_state_dict'])
    model.null_audio_embed.data = checkpoint['null_audio_embed'].to(device)
    model.null_text_embed.data = checkpoint['null_text_embed'].to(device)
    
    if 'unet_state_dict' in checkpoint:
        model.storyboard_unet.unet.load_state_dict(checkpoint['unet_state_dict'])
        print("   UNet state loaded")
    
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print("   Optimizer state loaded")
    
    epoch = checkpoint.get('epoch', 0)
    loss = checkpoint.get('loss', float('inf'))
    global_step = checkpoint.get('global_step', 0)
    
    print(f"âœ… Checkpoint loaded! Epoch: {epoch}, Loss: {loss:.4f}")
    return epoch, loss, global_step


def get_scheduler_with_warmup(optimizer, warmup_steps, total_steps):
    """Warmup + Cosine Annealing Scheduler"""
    def lr_lambda(current_step):
        if warmup_steps > 0 and current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))
    
    return LambdaLR(optimizer, lr_lambda)


def normalize_mel(mel):
    """
    Mel-spectrogram ì •ê·œí™”
    ì „ì²˜ë¦¬ì—ì„œ power_to_db(ref=np.max) ì ìš© â†’ ëŒ€ëµ [-80, 0] ë²”ìœ„
    ì´ë¥¼ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
    """
    # ê°’ ë²”ìœ„ í™•ì¸
    mel_min = mel.min()
    mel_max = mel.max()
    
    # dB scaleë¡œ ë³´ì´ë©´ ì •ê·œí™” (minì´ -50 ì´í•˜)
    if mel_min < -50:
        # [-80, 0] â†’ [0, 1] â†’ [-1, 1]
        mel = (mel + 80) / 80  # [0, 1]
        mel = mel * 2 - 1       # [-1, 1]
    
    # ê·¹ë‹¨ê°’ í´ë¨í•‘
    mel = torch.clamp(mel, -5, 5)
    
    return mel


def check_tensor(tensor, name, step=None):
    """í…ì„œì˜ NaN/Inf ì²´í¬ ë° í†µê³„ ì¶œë ¥"""
    has_nan = torch.isnan(tensor).any().item()
    has_inf = torch.isinf(tensor).any().item()
    
    if has_nan or has_inf:
        step_str = f" at step {step}" if step else ""
        print(f"âš ï¸ [{name}]{step_str}: NaN={has_nan}, Inf={has_inf}")
        print(f"   Shape: {tensor.shape}")
        print(f"   Stats: min={tensor.min():.4f}, max={tensor.max():.4f}, mean={tensor.mean():.4f}")
        return False
    return True


def diagnose_nan_location(model, mel, latent, text_embed, mel_mask, device):
    """NaN ë°œìƒ ìœ„ì¹˜ë¥¼ ì§„ë‹¨í•˜ëŠ” í•¨ìˆ˜"""
    print("\n" + "=" * 60)
    print("ğŸ” NaN ë°œìƒ ìœ„ì¹˜ ì§„ë‹¨ ì‹œì‘")
    print("=" * 60)
    
    with torch.no_grad():
        # 1. ì…ë ¥ ë°ì´í„° ì²´í¬
        print("\n[1] ì…ë ¥ ë°ì´í„° ì²´í¬:")
        print(f"   mel: min={mel.min():.4f}, max={mel.max():.4f}, mean={mel.mean():.4f}")
        print(f"   latent: min={latent.min():.4f}, max={latent.max():.4f}, mean={latent.mean():.4f}")
        print(f"   text_embed: min={text_embed.min():.4f}, max={text_embed.max():.4f}, mean={text_embed.mean():.4f}")
        
        # 2. Mel ì •ê·œí™” í›„ ì²´í¬
        mel_norm = normalize_mel(mel.clone())
        print(f"\n[2] Mel ì •ê·œí™” í›„:")
        print(f"   mel_norm: min={mel_norm.min():.4f}, max={mel_norm.max():.4f}, mean={mel_norm.mean():.4f}")
        
        # 3. Audio Encoder ë‹¨ê³„ë³„ ì²´í¬
        print("\n[3] Audio Encoder ë‹¨ê³„ë³„ ì²´í¬:")
        
        audio_encoder = model.audio_encoder
        B, n_mels, T = mel_norm.shape
        
        # CNN
        x = mel_norm.unsqueeze(1)  # [B, 1, 128, T]
        
        # Input BatchNorm (ìˆë‹¤ë©´)
        if hasattr(audio_encoder, 'input_norm'):
            x = audio_encoder.input_norm(x)
            print(f"   After input_norm: min={x.min():.4f}, max={x.max():.4f}")
        
        # CNN layers
        x = audio_encoder.cnn(x)
        print(f"   After CNN: min={x.min():.4f}, max={x.max():.4f}, shape={x.shape}")
        
        if torch.isnan(x).any():
            print("   âŒ NaN detected after CNN!")
            return "CNN"
        
        # Reshape & Project
        B, C, H, T_new = x.shape
        x = x.permute(0, 3, 1, 2).reshape(B, T_new, C * H)
        x = audio_encoder.proj(x)
        print(f"   After proj: min={x.min():.4f}, max={x.max():.4f}, shape={x.shape}")
        
        if torch.isnan(x).any():
            print("   âŒ NaN detected after projection!")
            return "Projection"
        
        # Positional encoding
        x = audio_encoder.audio_pos_encoder(x)
        print(f"   After pos_enc: min={x.min():.4f}, max={x.max():.4f}")
        
        if torch.isnan(x).any():
            print("   âŒ NaN detected after positional encoding!")
            return "PositionalEncoding"
        
        # Transformer
        transformer_mask = None
        if mel_mask is not None:
            transformer_mask = audio_encoder._downsample_mask(mel_mask, T_new)
        
        x = audio_encoder.transformer(x, src_key_padding_mask=transformer_mask)
        print(f"   After transformer: min={x.min():.4f}, max={x.max():.4f}")
        
        if torch.isnan(x).any():
            print("   âŒ NaN detected after Transformer!")
            return "Transformer"
        
        # Segment Cross-Attention
        audio_segments = audio_encoder._split_audio_segments(x, transformer_mask)
        
        segment_outputs = []
        for i in range(audio_encoder.num_segments):
            audio_seg, seg_mask = audio_segments[i]
            query = audio_encoder.segment_queries[i].expand(B, -1, -1)
            query = query + audio_encoder.segment_embed[:, i:i+1, :]
            
            attn_out, _ = audio_encoder.segment_cross_attn[i](
                query=query,
                key=audio_seg,
                value=audio_seg,
                key_padding_mask=seg_mask
            )
            
            if torch.isnan(attn_out).any():
                print(f"   âŒ NaN detected in segment {i} cross-attention!")
                return f"CrossAttention_Segment{i}"
            
            segment_outputs.append(attn_out)
            print(f"   Segment {i}: min={attn_out.min():.4f}, max={attn_out.max():.4f}")
        
        # Combine & Global attention
        combined = torch.cat(segment_outputs, dim=1)
        combined = audio_encoder.query_pos_encoder(combined)
        
        refined, _ = audio_encoder.global_attn(combined, combined, combined)
        combined = combined + refined
        print(f"   After global_attn: min={combined.min():.4f}, max={combined.max():.4f}")
        
        if torch.isnan(combined).any():
            print("   âŒ NaN detected after global attention!")
            return "GlobalAttention"
        
        # Output projection
        output = audio_encoder.output_proj(combined)
        print(f"   After output_proj: min={output.min():.4f}, max={output.max():.4f}")
        
        if torch.isnan(output).any():
            print("   âŒ NaN detected in final output!")
            return "OutputProjection"
        
        print("\nâœ… Audio Encoder ì „ì²´ ì •ìƒ!")
        print("=" * 60 + "\n")
        return None


def train(args):
    # Config ë¡œë“œ
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ Using device: {device}")
    
    # Dataset
    train_dataset = PreprocessedStoryboardDataset(
        features_dir=config['data']['features_dir'],
        split='train',
        max_mel_length=config['data']['max_mel_length']
    )
    
    val_dataset = PreprocessedStoryboardDataset(
        features_dir=config['data']['features_dir'],
        split='val',
        max_mel_length=config['data']['max_mel_length']
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=2,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=False,
        num_workers=2,
        collate_fn=collate_fn
    )
    
    # Model - Pipelineì— ì •ì˜ëœ íŒŒë¼ë¯¸í„°ë§Œ ì „ë‹¬
    model = AudioToStoryboardPipeline(
        pretrained_model=config['model']['pretrained_model'],
        audio_encoder_config=config['model']['audio_encoder'],
        freeze_unet=config['model'].get('freeze_unet', True),
        align_weight=config['model'].get('align_weight', 1.0),
    ).to(device)
    
    # Trainable parameters
    trainable_params = list(model.audio_encoder.parameters())
    trainable_params += [model.null_audio_embed]
    trainable_params += [model.null_text_embed]
    
    if not config['model'].get('freeze_unet', True):
        trainable_params += list(model.storyboard_unet.unet.parameters())
    
    optimizer = AdamW(
        trainable_params,
        lr=config['training']['learning_rate'],
        weight_decay=0.01
    )
    
    # Gradient accumulation
    grad_accum_steps = config['training']['gradient_accumulation_steps']
    steps_per_epoch = len(train_loader) // grad_accum_steps
    total_steps = steps_per_epoch * config['training']['num_epochs']
    
    # Mixed precision
    use_amp = config.get('mixed_precision', 'fp32') == 'fp16'
    scaler = torch.amp.GradScaler('cuda') if use_amp else None
    
    # Output dir
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # ============================================
    # ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ
    # ============================================
    start_epoch = 0
    global_step = 0
    best_val_loss = float('inf')
    
    if args.resume:
        checkpoint_path = args.resume if os.path.isfile(args.resume) else os.path.join(config['output_dir'], args.resume)
        if os.path.exists(checkpoint_path):
            start_epoch, _, global_step = load_checkpoint(checkpoint_path, model, optimizer, device)
            start_epoch += 1
            if global_step == 0:
                global_step = start_epoch * steps_per_epoch
            print(f"ğŸ”„ Resuming from epoch {start_epoch}, step {global_step}")
        else:
            print(f"âš ï¸ Checkpoint not found: {checkpoint_path}")
    
    elif args.auto_resume:
        cp_epoch = scan_checkpoint(config['output_dir'], 'checkpoint_epoch_')
        cp_step = scan_checkpoint(config['output_dir'], 'checkpoint_step_')
        
        checkpoint_path = None
        if cp_epoch and cp_step:
            checkpoint_path = cp_epoch if os.path.getmtime(cp_epoch) > os.path.getmtime(cp_step) else cp_step
        elif cp_epoch:
            checkpoint_path = cp_epoch
        elif cp_step:
            checkpoint_path = cp_step
        
        if checkpoint_path:
            start_epoch, _, global_step = load_checkpoint(checkpoint_path, model, optimizer, device)
            start_epoch += 1
            if global_step == 0:
                global_step = start_epoch * steps_per_epoch
            print(f"ğŸ”„ Auto-resuming from epoch {start_epoch}, step {global_step}")
        else:
            print("ğŸ“‚ No checkpoint found. Starting from scratch...")
    
    # Scheduler
    scheduler = get_scheduler_with_warmup(
        optimizer,
        warmup_steps=config['training'].get('warmup_steps', 0),
        total_steps=total_steps
    )
    for _ in range(global_step):
        scheduler.step()
    
    # Wandb
    wandb.init(project="audio-to-storyboard", config=config, resume="allow")
    
    # Conditioning mode
    conditioning_mode = config['training'].get('conditioning_mode', 'both')
    print(f"ğŸ¯ Conditioning mode: {conditioning_mode}")
    print(f"ğŸ“Š Align Weight: {model.align_weight}")
    
    # íŒŒë¼ë¯¸í„° ìŠ¤ëƒ…ìƒ·
    import copy
    initial_params = {
        name: param.clone().detach().cpu() 
        for name, param in model.audio_encoder.named_parameters()
    }
    initial_null_audio = model.null_audio_embed.clone().detach().cpu()
    initial_null_text = model.null_text_embed.clone().detach().cpu()
    print("ğŸ“¸ Initial parameter snapshot saved")
    
    # ============================================
    # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ NaN ì§„ë‹¨ (ë””ë²„ê·¸ ëª¨ë“œ)
    # ============================================
    if args.diagnose:
        print("\nğŸ” Diagnose mode enabled - checking first batch...")
        first_batch = next(iter(train_loader))
        mel = first_batch['mel'].to(device)
        latent = first_batch['latent'].to(device)
        text_embed = first_batch['text_embed'].to(device)
        mel_mask = first_batch['mel_mask'].to(device)
        
        nan_location = diagnose_nan_location(model, mel, latent, text_embed, mel_mask, device)
        
        if nan_location:
            print(f"\nâŒ NaN ë°œìƒ ìœ„ì¹˜: {nan_location}")
            print("   í•´ë‹¹ ë¶€ë¶„ì„ ìˆ˜ì • í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            return
        else:
            print("\nâœ… ì§„ë‹¨ ì™„ë£Œ - NaN ì—†ìŒ. í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # Training loop
    print(f"\nğŸš€ Training: Epoch {start_epoch + 1} ~ {config['training']['num_epochs']}")
    print(f"   Steps per epoch: {steps_per_epoch}, Starting step: {global_step}")
    
    nan_count = 0
    max_nan_count = 50
    diagnosed_nan = False  # ì²« NaN ë°œìƒ ì‹œ í•œ ë²ˆë§Œ ì§„ë‹¨
    
    for epoch in range(start_epoch, config['training']['num_epochs']):
        model.train()
        epoch_loss = 0.0
        valid_batches = 0
        optimizer.zero_grad()
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['training']['num_epochs']}")
        
        for batch_idx, batch in enumerate(pbar):
            mel = batch['mel'].to(device)
            latent = batch['latent'].to(device)
            text_embed = batch['text_embed'].to(device)
            mel_mask = batch['mel_mask'].to(device)
            
            # ============================================
            # ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”
            # ============================================
            if torch.isnan(mel).any() or torch.isinf(mel).any():
                print(f"âš ï¸ NaN/Inf in mel at batch {batch_idx}, skipping...")
                continue
            if torch.isnan(latent).any() or torch.isinf(latent).any():
                print(f"âš ï¸ NaN/Inf in latent at batch {batch_idx}, skipping...")
                continue
            if torch.isnan(text_embed).any() or torch.isinf(text_embed).any():
                print(f"âš ï¸ NaN/Inf in text_embed at batch {batch_idx}, skipping...")
                continue
            
            # ğŸ”¥ Mel ì •ê·œí™” ì ìš©
            mel = normalize_mel(mel)
            
            # Forward
            with torch.amp.autocast('cuda', enabled=use_amp):
                output = model(
                    mel=mel,
                    latent=latent,
                    text_embed=text_embed,
                    mel_mask=mel_mask,
                    conditioning_mode=conditioning_mode
                )
                loss = output['loss'] / grad_accum_steps
            
            # ============================================
            # NaN Loss ì²˜ë¦¬
            # ============================================
            if torch.isnan(loss) or torch.isinf(loss):
                nan_count += 1
                print(f"âš ï¸ NaN loss at step {global_step} (count: {nan_count}/{max_nan_count})")
                
                # ì²« NaN ë°œìƒ ì‹œ ì§„ë‹¨ ì‹¤í–‰
                if not diagnosed_nan:
                    diagnosed_nan = True
                    nan_location = diagnose_nan_location(model, mel, latent, text_embed, mel_mask, device)
                    if nan_location:
                        print(f"   NaN ë°œìƒ ìœ„ì¹˜: {nan_location}")
                
                optimizer.zero_grad()
                
                if nan_count >= max_nan_count:
                    print("âŒ Too many NaN losses, stopping!")
                    save_checkpoint(
                        model, optimizer, epoch, float('inf'),
                        os.path.join(config['output_dir'], f'checkpoint_nan_stop_{global_step}.pt'),
                        config['model'].get('freeze_unet', True), global_step
                    )
                    wandb.finish()
                    return
                continue
            else:
                nan_count = 0
            
            # Backward
            if scaler is not None:
                scaler.scale(loss).backward()
            else:
                loss.backward()
            
            epoch_loss += output['loss'].item()
            valid_batches += 1
            
            # Gradient accumulation step
            if (batch_idx + 1) % grad_accum_steps == 0:
                if scaler is not None:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(trainable_params, 0.5)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(trainable_params, 0.5)
                    optimizer.step()
                
                scheduler.step()
                optimizer.zero_grad()
                global_step += 1
                
                # Logging
                if global_step % 10 == 0:
                    log_dict = {
                        'train/loss': output['loss'].item(),
                        'train/lr': scheduler.get_last_lr()[0],
                        'train/epoch': epoch
                    }
                    if 'diffusion_loss' in output:
                        diff_loss = output['diffusion_loss']
                        log_dict['train/diffusion_loss'] = diff_loss.item() if torch.is_tensor(diff_loss) else diff_loss
                    if 'align_loss' in output:
                        align_loss = output['align_loss']
                        log_dict['train/align_loss'] = align_loss.item() if torch.is_tensor(align_loss) else align_loss
                    wandb.log(log_dict, step=global_step)
                
                # Checkpoint
                if global_step % args.checkpoint_interval == 0 and global_step > 0:
                    save_checkpoint(
                        model, optimizer, epoch, output['loss'].item(),
                        os.path.join(config['output_dir'], f'checkpoint_step_{global_step}.pt'),
                        config['model'].get('freeze_unet', True), global_step
                    )
                    print(f"ğŸ’¾ Checkpoint saved at step {global_step}")
            
            # Progress bar
            postfix = {'loss': f"{output['loss'].item():.4f}", 'lr': f"{scheduler.get_last_lr()[0]:.2e}"}
            if 'align_loss' in output:
                align_val = output['align_loss']
                align_val = align_val.item() if torch.is_tensor(align_val) else align_val
                postfix['align'] = f"{align_val:.4f}"
            pbar.set_postfix(postfix)
        
        # Epoch end
        avg_train_loss = epoch_loss / max(valid_batches, 1)
        print(f"ğŸ“Š Epoch {epoch+1} - Avg Train Loss: {avg_train_loss:.4f}")
        
        # Validation
        if (epoch + 1) % config['training']['eval_every'] == 0:
            val_loss = validate(model, val_loader, device, scaler, conditioning_mode)
            print(f"ğŸ“Š Epoch {epoch+1} - Val Loss: {val_loss:.4f}")
            
            wandb.log({'val/loss': val_loss, 'val/epoch': epoch}, step=global_step)
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                save_checkpoint(
                    model, optimizer, epoch, val_loss,
                    os.path.join(config['output_dir'], 'best_model.pt'),
                    config['model'].get('freeze_unet', True), global_step
                )
                print(f"ğŸ’¾ Best model saved!")
        
        # Periodic save
        if (epoch + 1) % config['training']['save_every'] == 0:
            save_checkpoint(
                model, optimizer, epoch, avg_train_loss,
                os.path.join(config['output_dir'], f'checkpoint_epoch_{epoch+1}.pt'),
                config['model'].get('freeze_unet', True), global_step
            )
    
    # ============================================
    # íŒŒë¼ë¯¸í„° ë³€í™” ë¶„ì„
    # ============================================
    print("\n" + "=" * 60)
    print("ğŸ“Š Audio Encoder íŒŒë¼ë¯¸í„° ë³€í™” ë¶„ì„")
    print("=" * 60)
    
    total_change = 0
    total_params = 0
    layer_changes = []
    
    for name, param in model.audio_encoder.named_parameters():
        if name in initial_params:
            change = (param.detach().cpu() - initial_params[name]).abs()
            mean_change = change.mean().item()
            max_change = change.max().item()
            layer_changes.append((name, mean_change, max_change))
            total_change += change.sum().item()
            total_params += param.numel()
    
    layer_changes.sort(key=lambda x: x[1], reverse=True)
    print("\nğŸ” Top 5 ë³€í™” ë ˆì´ì–´:")
    for name, mean_c, max_c in layer_changes[:5]:
        print(f"   {name}: mean={mean_c:.6f}, max={max_c:.6f}")
    
    null_audio_change = (model.null_audio_embed.detach().cpu() - initial_null_audio).abs().mean().item()
    null_text_change = (model.null_text_embed.detach().cpu() - initial_null_text).abs().mean().item()
    print(f"\nğŸ“ Null Audio Embed ë³€í™”: {null_audio_change:.6f}")
    print(f"ğŸ“ Null Text Embed ë³€í™”: {null_text_change:.6f}")
    
    avg_change = total_change / max(total_params, 1)
    print(f"\nğŸ“ˆ ì „ì²´ í‰ê·  íŒŒë¼ë¯¸í„° ë³€í™”: {avg_change:.8f}")
    
    if avg_change < 1e-8:
        print("âš ï¸ ê²½ê³ : íŒŒë¼ë¯¸í„°ê°€ ê±°ì˜ ë³€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    elif avg_change < 1e-5:
        print("âœ… íŒŒë¼ë¯¸í„°ê°€ ì¡°ê¸ˆ ë³€í–ˆìŠµë‹ˆë‹¤. í•™ìŠµì´ ì²œì²œíˆ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.")
    else:
        print("âœ… íŒŒë¼ë¯¸í„°ê°€ ì¶©ë¶„íˆ ë³€í–ˆìŠµë‹ˆë‹¤. í•™ìŠµì´ ì˜ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    print("=" * 60)
    print("\nâœ… Training complete!")
    wandb.finish()


def save_checkpoint(model, optimizer, epoch, loss, path, freeze_unet, global_step=0):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    checkpoint = {
        'epoch': epoch,
        'global_step': global_step,
        'audio_encoder_state_dict': model.audio_encoder.state_dict(),
        'null_audio_embed': model.null_audio_embed.data.cpu(),
        'null_text_embed': model.null_text_embed.data.cpu(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss
    }
    
    if not freeze_unet:
        checkpoint['unet_state_dict'] = model.storyboard_unet.unet.state_dict()
    
    torch.save(checkpoint, path)


@torch.no_grad()
def validate(model, val_loader, device, scaler=None, conditioning_mode="both"):
    model.eval()
    total_loss = 0.0
    valid_batches = 0
    use_amp = scaler is not None
    
    for batch in tqdm(val_loader, desc="Validation"):
        mel = batch['mel'].to(device)
        latent = batch['latent'].to(device)
        text_embed = batch['text_embed'].to(device)
        mel_mask = batch['mel_mask'].to(device)
        
        if torch.isnan(mel).any() or torch.isnan(latent).any() or torch.isnan(text_embed).any():
            continue
        
        # Mel ì •ê·œí™”
        mel = normalize_mel(mel)
        
        with torch.amp.autocast('cuda', enabled=use_amp):
            output = model(
                mel=mel,
                latent=latent,
                text_embed=text_embed,
                mel_mask=mel_mask,
                conditioning_mode=conditioning_mode
            )
        
        loss = output['loss']
        
        if torch.isnan(loss) or torch.isinf(loss):
            continue
        
        total_loss += loss.item()
        valid_batches += 1
    
    if valid_batches == 0:
        print("âš ï¸ All validation batches had NaN loss!")
        return float('inf')
    
    skipped = len(val_loader) - valid_batches
    if skipped > 0:
        print(f"âš ï¸ Skipped {skipped}/{len(val_loader)} validation batches due to NaN/Inf")
    
    return total_loss / valid_batches


def main():
    print("ğŸ¬ Audio-to-Storyboard Training")
    print("=" * 50)
    
    parser = argparse.ArgumentParser(description='Audio-to-Storyboard Training')
    
    parser.add_argument('--config', type=str, default='configs/train_config.yaml',
                        help='Path to config file')
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to checkpoint file to resume from')
    parser.add_argument('--auto_resume', action='store_true',
                        help='Automatically find and resume from latest checkpoint')
    parser.add_argument('--checkpoint_interval', type=int, default=50,
                        help='Save checkpoint every N steps (default: 50)')
    parser.add_argument('--diagnose', action='store_true',
                        help='Run NaN diagnosis on first batch before training')
    
    args = parser.parse_args()
    
    print(f"ğŸ“„ Config: {args.config}")
    if args.resume:
        print(f"ğŸ”„ Resume from: {args.resume}")
    elif args.auto_resume:
        print(f"ğŸ”„ Auto-resume enabled")
    print(f"ğŸ’¾ Checkpoint interval: {args.checkpoint_interval} steps")
    if args.diagnose:
        print(f"ğŸ” Diagnose mode: enabled")
    print("=" * 50)
    
    train(args)


if __name__ == "__main__":
    main()