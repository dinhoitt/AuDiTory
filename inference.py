# inference.py
# Audio-to-Storyboard Inference Script

import torch
import torch.nn.functional as F
import numpy as np
import argparse
import os
import yaml
from PIL import Image

# Audio processing
import librosa

# CLIP (for text conditioning)
try:
    from transformers import CLIPTextModel, CLIPTokenizer
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("âš ï¸ transformers not installed. Text conditioning will not be available.")

from models.pipeline import AudioToStoryboardPipeline


# ============================================
# Mel-Spectrogram ì„¤ì • (ì „ì²˜ë¦¬ì™€ ë™ì¼)
# ============================================
MEL_CONFIG = {
    'sr': 24000,
    'n_mels': 128,
    'hop_length': 512,
    'n_fft': 2048,
    'fmin': 0,
    'fmax': 12000
}


def load_checkpoint(checkpoint_path: str, model: AudioToStoryboardPipeline, device: str = 'cuda'):
    """
    ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    
    Args:
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        model: AudioToStoryboardPipeline ì¸ìŠ¤í„´ìŠ¤
        device: ë””ë°”ì´ìŠ¤
    
    Returns:
        ë¡œë“œëœ ëª¨ë¸
    """
    print(f"ğŸ“‚ Loading checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # Audio Encoder state ë¡œë“œ
    model.audio_encoder.load_state_dict(checkpoint['audio_encoder_state_dict'])
    
    # Null embeddings ë¡œë“œ (device ëª…ì‹œ)
    model.null_audio_embed.data = checkpoint['null_audio_embed'].to(device)
    if 'null_text_embed' in checkpoint:
        model.null_text_embed.data = checkpoint['null_text_embed'].to(device)
    
    # UNet state ë¡œë“œ (fine-tuningëœ ê²½ìš°)
    if 'unet_state_dict' in checkpoint:
        model.storyboard_unet.unet.load_state_dict(checkpoint['unet_state_dict'])
        print("   âœ“ UNet state loaded (fine-tuned)")
    
    epoch = checkpoint.get('epoch', 'unknown')
    loss = checkpoint.get('loss', float('inf'))
    print(f"âœ… Checkpoint loaded! Epoch: {epoch}, Loss: {loss:.4f}")
    
    return model


def load_audio_to_mel(
    audio_path: str, 
    max_length: int = 2048, 
    device: str = 'cuda'
) -> tuple:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ mel-spectrogramìœ¼ë¡œ ë³€í™˜ (ì „ì²˜ë¦¬ì™€ ë™ì¼í•œ ë°©ì‹)
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        max_length: ìµœëŒ€ ì‹œê°„ í”„ë ˆì„ ìˆ˜
        device: ë””ë°”ì´ìŠ¤
    
    Returns:
        mel: [1, 128, max_length] í…ì„œ
        mel_mask: [1, max_length] bool í…ì„œ (True = íŒ¨ë”© ìœ„ì¹˜)
    """
    print(f"ğŸµ Loading audio: {audio_path}")
    
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")
    
    # ì˜¤ë””ì˜¤ ë¡œë“œ (ì „ì²˜ë¦¬ì™€ ë™ì¼í•œ sample rate)
    y, sr = librosa.load(audio_path, sr=MEL_CONFIG['sr'])
    
    # ìµœì†Œ ê¸¸ì´ ë³´ì¥ (1ì´ˆ) - ì „ì²˜ë¦¬ì™€ ë™ì¼
    min_samples = MEL_CONFIG['sr']
    if len(y) < min_samples:
        y = np.pad(y, (0, min_samples - len(y)))
    
    # Mel-spectrogram ê³„ì‚° (ì „ì²˜ë¦¬ì™€ ë™ì¼)
    mel = librosa.feature.melspectrogram(
        y=y,
        sr=sr,
        n_mels=MEL_CONFIG['n_mels'],
        hop_length=MEL_CONFIG['hop_length'],
        n_fft=MEL_CONFIG['n_fft'],
        fmin=MEL_CONFIG['fmin'],
        fmax=MEL_CONFIG['fmax']
    )
    
    # Log scale ë³€í™˜ (ì „ì²˜ë¦¬ì™€ ë™ì¼ - ì •ê·œí™” ì—†ìŒ!)
    mel_db = librosa.power_to_db(mel, ref=np.max)
    
    # ì›ë³¸ ê¸¸ì´ ì €ì¥
    original_len = mel_db.shape[1]
    
    # Padding/Truncation (Datasetê³¼ ë™ì¼í•œ ë°©ì‹)
    if original_len > max_length:
        mel_db = mel_db[:, :max_length]
        original_len = max_length
    else:
        pad_len = max_length - original_len
        mel_db = np.pad(mel_db, ((0, 0), (0, pad_len)), mode='constant', constant_values=0)
    
    # Mask ìƒì„± (True = íŒ¨ë”© ìœ„ì¹˜, Datasetê³¼ ë™ì¼)
    mel_mask = np.zeros(max_length, dtype=bool)
    mel_mask[original_len:] = True
    
    # Tensor ë³€í™˜
    mel = torch.from_numpy(mel_db).float().unsqueeze(0).to(device)      # [1, 128, max_length]
    mel_mask = torch.from_numpy(mel_mask).unsqueeze(0).to(device)       # [1, max_length]
    
    duration = len(y) / sr
    print(f"   âœ“ Mel shape: {mel.shape}, Duration: {duration:.2f}s, Original frames: {original_len}")
    
    return mel, mel_mask


def load_text_to_embed(
    text: str, 
    pretrained_model: str = "runwayml/stable-diffusion-v1-5",
    device: str = 'cuda'
) -> torch.Tensor:
    """
    í…ìŠ¤íŠ¸ë¥¼ CLIP embeddingìœ¼ë¡œ ë³€í™˜ (ì „ì²˜ë¦¬ì™€ ë™ì¼í•œ ë°©ì‹)
    
    Args:
        text: í…ìŠ¤íŠ¸ ë¬¸ìì—´
        pretrained_model: SD ëª¨ë¸ ê²½ë¡œ (tokenizer, text_encoder ì‚¬ìš©)
        device: ë””ë°”ì´ìŠ¤
    
    Returns:
        text_embed: [1, 77, 768] í…ì„œ
    """
    if not CLIP_AVAILABLE:
        raise RuntimeError("transformers package required for text conditioning. "
                          "Install with: pip install transformers")
    
    print(f"ğŸ“ Encoding text: '{text[:50]}{'...' if len(text) > 50 else ''}'")
    
    # SDì˜ tokenizerì™€ text_encoder ì‚¬ìš© (ì „ì²˜ë¦¬ì™€ ë™ì¼)
    tokenizer = CLIPTokenizer.from_pretrained(pretrained_model, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(pretrained_model, subfolder="text_encoder").to(device)
    text_encoder.eval()
    
    # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ì „ì²˜ë¦¬ì™€ ë™ì¼)
    if not text:
        text = ""
    
    # í† í°í™” ë° ì¸ì½”ë”© (ì „ì²˜ë¦¬ì™€ ë™ì¼)
    inputs = tokenizer(
        text,
        padding="max_length",
        max_length=tokenizer.model_max_length,  # 77
        truncation=True,
        return_tensors="pt"
    ).to(device)
    
    with torch.no_grad():
        text_embed = text_encoder(**inputs).last_hidden_state  # [1, 77, 768]
    
    print(f"   âœ“ Text embed shape: {text_embed.shape}")
    
    return text_embed


def save_storyboard(
    images: torch.Tensor, 
    output_dir: str, 
    prefix: str = "storyboard"
) -> list:
    """
    ìƒì„±ëœ ìŠ¤í† ë¦¬ë³´ë“œ ì´ë¯¸ì§€ ì €ì¥
    
    Args:
        images: [B, 4, 3, H, W] í…ì„œ (ê°’ ë²”ìœ„: 0~1)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        prefix: íŒŒì¼ëª… prefix
    
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    os.makedirs(output_dir, exist_ok=True)
    
    B = images.shape[0]
    saved_files = []
    
    for batch_idx in range(B):
        batch_images = images[batch_idx]  # [4, 3, H, W]
        
        # ê°œë³„ í”„ë ˆì„ ì €ì¥
        frame_files = []
        for frame_idx in range(4):
            img = batch_images[frame_idx]  # [3, H, W]
            img = img.permute(1, 2, 0).cpu().numpy()  # [H, W, 3]
            img = np.clip(img * 255, 0, 255).astype(np.uint8)
            
            pil_img = Image.fromarray(img)
            filename = f"{prefix}_{batch_idx:03d}_frame{frame_idx}.png"
            filepath = os.path.join(output_dir, filename)
            pil_img.save(filepath)
            frame_files.append(filepath)
        
        # 4í”„ë ˆì„ ê·¸ë¦¬ë“œ ì €ì¥ (2x2 ë°°ì—´)
        H, W = batch_images.shape[2], batch_images.shape[3]
        grid_img = Image.new('RGB', (W * 2, H * 2))
        
        for frame_idx in range(4):
            img = batch_images[frame_idx]
            img = img.permute(1, 2, 0).cpu().numpy()
            img = np.clip(img * 255, 0, 255).astype(np.uint8)
            pil_img = Image.fromarray(img)
            
            # 2x2 ê·¸ë¦¬ë“œ ìœ„ì¹˜ ê³„ì‚°
            row, col = frame_idx // 2, frame_idx % 2
            grid_img.paste(pil_img, (col * W, row * H))
        
        grid_filename = f"{prefix}_{batch_idx:03d}_grid.png"
        grid_filepath = os.path.join(output_dir, grid_filename)
        grid_img.save(grid_filepath)
        
        # ê°€ë¡œ ë°°ì—´ ê·¸ë¦¬ë“œë„ ì €ì¥
        horizontal_img = Image.new('RGB', (W * 4, H))
        for frame_idx in range(4):
            img = batch_images[frame_idx]
            img = img.permute(1, 2, 0).cpu().numpy()
            img = np.clip(img * 255, 0, 255).astype(np.uint8)
            pil_img = Image.fromarray(img)
            horizontal_img.paste(pil_img, (frame_idx * W, 0))
        
        horizontal_filename = f"{prefix}_{batch_idx:03d}_horizontal.png"
        horizontal_filepath = os.path.join(output_dir, horizontal_filename)
        horizontal_img.save(horizontal_filepath)
        
        print(f"ğŸ’¾ Saved: {grid_filename} (2x2), {horizontal_filename} (1x4)")
        saved_files.extend(frame_files + [grid_filepath, horizontal_filepath])
    
    return saved_files


def inference(args):
    """ì¶”ë¡  ì‹¤í–‰"""
    
    # Device ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  Using device: {device}")
    
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Config ë¡œë“œ
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    pretrained_model = config['model']['pretrained_model']
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("\nğŸ”§ Initializing model...")
    model = AudioToStoryboardPipeline(
        pretrained_model=pretrained_model,
        audio_encoder_config=config['model']['audio_encoder'],
        freeze_unet=config['model'].get('freeze_unet', True),
        align_weight=config['model'].get('align_weight', 0.1),
    ).to(device)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    model = load_checkpoint(args.checkpoint, model, device)
    model.eval()
    
    # ============================================
    # ì…ë ¥ ì¤€ë¹„
    # ============================================
    mel = None
    mel_mask = None
    text_embed = None
    
    # Audio ë¡œë“œ
    if args.audio:
        mel, mel_mask = load_audio_to_mel(
            args.audio,
            max_length=config['data'].get('max_mel_length', 2048),
            device=device
        )
    
    # Text ë¡œë“œ
    if args.text:
        text_embed = load_text_to_embed(
            args.text,
            pretrained_model=pretrained_model,
            device=device
        )
    
    # Conditioning mode ê²°ì •
    if mel is not None and text_embed is not None:
        conditioning_mode = "both"
    elif mel is not None:
        conditioning_mode = "audio"
    elif text_embed is not None:
        conditioning_mode = "text"
    else:
        raise ValueError("At least one of --audio or --text must be provided")
    
    print(f"\nğŸ¯ Conditioning mode: {conditioning_mode}")
    
    # ============================================
    # ì§„ë‹¨: Audio/Text Embedding í†µê³„ í™•ì¸
    # ============================================
    print("\n" + "=" * 60)
    print("ğŸ” Embedding ì§„ë‹¨")
    print("=" * 60)
    
    with torch.no_grad():
        if mel is not None:
            # Audio Encoder ì¶œë ¥ í™•ì¸
            audio_embeds = model.audio_encoder(mel, mel_mask)
            print(f"\nğŸ“Š [Audio Encoder ì¶œë ¥]")
            print(f"   Shape: {audio_embeds.shape}")
            print(f"   Mean:  {audio_embeds.mean().item():.4f}")
            print(f"   Std:   {audio_embeds.std().item():.4f}")
            print(f"   Max:   {audio_embeds.max().item():.4f}")
            print(f"   Min:   {audio_embeds.min().item():.4f}")
            
            # ë¬¸ì œ ì§„ë‹¨
            if torch.isnan(audio_embeds).any():
                print("   âš ï¸ WARNING: NaN detected in audio embedding!")
            if torch.isinf(audio_embeds).any():
                print("   âš ï¸ WARNING: Inf detected in audio embedding!")
            if audio_embeds.max().abs() > 50:
                print("   âš ï¸ WARNING: ê°’ í­ë°œ ê°€ëŠ¥ì„±! (Max > 50)")
            if audio_embeds.std() < 0.1:
                print("   âš ï¸ WARNING: ì¶œë ¥ ë¶•ê´´ ê°€ëŠ¥ì„±! (Std < 0.1)")
        
        if text_embed is not None:
            print(f"\nğŸ“Š [Text Embedding ì°¸ê³ ]")
            print(f"   Shape: {text_embed.shape}")
            print(f"   Mean:  {text_embed.mean().item():.4f}")
            print(f"   Std:   {text_embed.std().item():.4f}")
            print(f"   Max:   {text_embed.max().item():.4f}")
            print(f"   Min:   {text_embed.min().item():.4f}")
        
        # ë¹„êµ ë¶„ì„
        if mel is not None and text_embed is not None:
            print(f"\nğŸ“Š [ë¹„êµ ë¶„ì„]")
            audio_mean = audio_embeds.mean().item()
            audio_std = audio_embeds.std().item()
            text_mean = text_embed.mean().item()
            text_std = text_embed.std().item()
            
            mean_diff = abs(audio_mean - text_mean)
            std_ratio = audio_std / (text_std + 1e-8)
            
            print(f"   Mean ì°¨ì´: {mean_diff:.4f}")
            print(f"   Std ë¹„ìœ¨ (Audio/Text): {std_ratio:.4f}")
            
            if mean_diff > 1.0:
                print("   âš ï¸ Mean ì°¨ì´ê°€ í¼ - ë¶„í¬ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„±")
            if std_ratio > 3.0 or std_ratio < 0.3:
                print("   âš ï¸ Std ë¹„ìœ¨ ì´ìƒ - ë¶„í¬ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„±")
    
    print("=" * 60)
    
    # ============================================
    # ìƒì„±
    # ============================================
    print(f"\nğŸ¨ Generating storyboard...")
    print(f"   Steps: {args.steps}")
    print(f"   Guidance scale: {args.guidance}")
    
    # Generator ì„¤ì • (ì¬í˜„ì„±)
    generator = None
    if args.seed is not None:
        generator = torch.Generator(device=device).manual_seed(args.seed)
    
    # Consistent attention ì„¤ì • (ê¸°ë³¸ê°’: True)
    use_consistent = not getattr(args, 'no_consistent_attention', False)
    print(f"   Consistent attention: {'enabled' if use_consistent else 'disabled'}")
    
    with torch.no_grad():
        images = model.generate(
            mel=mel,
            text_embed=text_embed,
            num_inference_steps=args.steps,
            guidance_scale=args.guidance,
            conditioning_mode=conditioning_mode,
            generator=generator,
            use_consistent_attention=use_consistent,
        )
    
    print(f"âœ… Generated images shape: {images.shape}")  # [B, 4, 3, H, W]
    
    # ============================================
    # ì €ì¥
    # ============================================
    saved_files = save_storyboard(images, args.output, prefix=args.prefix)
    
    print(f"\nğŸ‰ Done! Results saved to: {args.output}")
    print(f"   Total files: {len(saved_files)}")
    
    return saved_files


def main():
    parser = argparse.ArgumentParser(
        description='Audio-to-Storyboard Inference',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Audio only
  python inference.py --checkpoint best_model.pt --audio music.wav
  
  # Text only
  python inference.py --checkpoint best_model.pt --text "A hero's journey"
  
  # Both audio and text
  python inference.py --checkpoint best_model.pt --audio music.wav --text "Epic adventure"
  
  # With custom settings
  python inference.py --checkpoint best_model.pt --audio music.wav \\
      --steps 100 --guidance 9.0 --seed 42 --output ./results
        """
    )
    
    # Required
    parser.add_argument('--checkpoint', type=str, required=True,
                        help='Path to model checkpoint (.pt file)')
    
    # Input (at least one required)
    parser.add_argument('--audio', type=str, default=None,
                        help='Path to audio file (wav, mp3, flac, etc.)')
    parser.add_argument('--text', type=str, default=None,
                        help='Text prompt for conditioning')
    
    # Config
    parser.add_argument('--config', type=str, default='configs/train_config.yaml',
                        help='Path to config file (default: configs/train_config.yaml)')
    
    # Generation settings
    parser.add_argument('--steps', type=int, default=50,
                        help='Number of denoising steps (default: 50)')
    parser.add_argument('--guidance', type=float, default=7.5,
                        help='CFG guidance scale (default: 7.5)')
    parser.add_argument('--no_consistent_attention', action='store_true',
                        help='Disable consistent self-attention (enabled by default)')
    
    # Output
    parser.add_argument('--output', type=str, default='./outputs',
                        help='Output directory (default: ./outputs)')
    parser.add_argument('--prefix', type=str, default='storyboard',
                        help='Output filename prefix (default: storyboard)')
    
    # Reproducibility
    parser.add_argument('--seed', type=int, default=None,
                        help='Random seed for reproducibility')
    
    args = parser.parse_args()
    
    # Validation
    if args.audio is None and args.text is None:
        parser.error("At least one of --audio or --text must be provided")
    
    # Seed ì„¤ì •
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(args.seed)
        print(f"ğŸ² Random seed set: {args.seed}")
    
    # ì •ë³´ ì¶œë ¥
    print("=" * 60)
    print("ğŸ¬ Audio-to-Storyboard Inference")
    print("=" * 60)
    print(f"ğŸ“„ Config:     {args.config}")
    print(f"ğŸ“¦ Checkpoint: {args.checkpoint}")
    if args.audio:
        print(f"ğŸµ Audio:      {args.audio}")
    if args.text:
        print(f"ğŸ“ Text:       {args.text}")
    print(f"ğŸ¨ Steps:      {args.steps}")
    print(f"ğŸ¯ Guidance:   {args.guidance}")
    print(f"ğŸ’¾ Output:     {args.output}")
    if args.seed is not None:
        print(f"ğŸ² Seed:       {args.seed}")
    print("=" * 60)
    
    # ì¶”ë¡  ì‹¤í–‰
    inference(args)


if __name__ == "__main__":
    main()